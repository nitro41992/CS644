pseudo-distributed:
-------------------

export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
PATH=$PATH:$HOME/.local/bin:$HOME/binPATH=$PATH:$JAVA_HOME/bin:PATH
export PATH
export HADOOP_HOME=/home/ubuntu/server/hadoop-2.10.0
export HADOOP_PREFIX=/home/ubuntu/server/hadoop-2.10.0
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
source ~/.bashrc

sudo nano /etc/hosts
	172.31.42.214 master
Ctrl + S
Ctrl + D

cd ~/server/hadoop-2.10.0/etc/hadoop/

sudo nano hadoop-env.sh
	# The java implementation to use.
	export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
	export HADOOP_HOME=/home/ubuntu/server/hadoop-2.10.0

	# The jsvc implementation to use. Jsvc is required to run secure datanodes
	# that bind to privileged ports to provide authentication of data transfer
	# protocol.  Jsvc is not required if SASL is configured for authentication of
	# data transfer protocol using non-privileged ports.
	#export JSVC_HOME=${JSVC_HOME}

	export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

	# Extra Java CLASSPATH elements.  Automatically insert capacity-scheduler.
	for file in /home/ubuntu/server/hadoop-2.10.0/share/hadoop/*/*.jar

	do

		export CLASSPATH=$CLASSPATH:$file

	done

	for file in /home/ubuntu/server/hadoop-2.10.0/share/hadoop/*/lib/*.jar

	do

		export CLASSPATH=$CLASSPATH:$file

	done
Ctrl + S
Ctrl + D

sudo nano core-site.xml
	<configuration>
		<property>
			<name>fs.defaultFS</name>
			<value>hdfs://localhost:9000</value>
		</property>
	</configuration>
Ctrl + S
Ctrl + D

sudo nano hdfs-site.xml
	<configuration>
		<property>
			<name>dfs.name.dir</name>
			<value>/home/ubuntu/server/hadoop-2.10.0/dfs/name</value>
			<final>true</final>
		</property>
		<property>
			<name>dfs.data.dir</name>
			<value>/home/ubuntu/server/hadoop-2.10.0/dfs/name/data</value>
			<final>true</final>
		</property>
		<property>
			<name>dfs.replication</name>
			<value>3</value>
		</property>
	</configuration>
Ctrl + S
Ctrl + D

sudo nano mapred-site.xml
	<configuration>
		<property> 
			<name>mapred.job.tracker</name> 
			<value>master:9001</value> 
		</property>
		<property> 
			<name>mapred.framework.name</name> 
			<value>yarn</value> 
		</property>
	</configuration>
Ctrl + S
Ctrl + D

sudo nano yarn-site.xml
	<configuration>
		<property>
			<name>yarn.nodemanager.aux-services</name>
			<value>mapreduce_shuffle</value>
		</property>
		<property>
			<name>yarn.nodemanager.aux-services.shuffle.class</name>
			<value>org.apache.hadoop.mapred.ShuffleHandler</value>
		</property>
		<property>
			<name>yarn.resourcemanager.address</name>
			<value>master:8032</value>
		</property>
		<property>
			<name>yarn.resourcemanager.scheduler.address</name>
			<value>master:8030</value>
		</property>
		<property>
			<name>yarn.resourcemanager.resource-tracker.address</name>
			<value>master:8035</value>
		</property>
		<property>
			<name>yarn.resourcemanager.admin.address</name>
			<value>master:8033</value>
		</property>
		<property>
			<name>yarn.resourcemanager.webapp.address</name>
			<value>master:8088</value>
		</property>
		<property>
			<name>yarn.resourcemanager.scheduler.class</name>
			<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
		</property>
		<property>
			<name>yarn.nodemanager.vmem-check-enabled</name>
			<value>false</value>
		</property>
		<property>
			<name>yarn.resourcemanager.hostname</name>
			<value>master</value>
		</property>
	</configuration>
Ctrl + S
Ctrl + D

hdfs namenode -format
hadoop dfsadmin -safemode leave

start-dfs.sh
start-yarn.sh

hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/ubuntu
hdfs dfs -mkdir /user/ubuntu/input

hdfs dfs -put ~/server/hadoop-2.10.0/repo/hw4/input/Input.txt /user/ubuntu/input/

hadoop jar RelFrequency.jar /user/ubuntu/input/ /user/ubuntu/output
ubuntu@ip-172-31-42-214:~/server/hadoop-2.10.0/repo/hw4$ hdfs dfs -get /user/ubuntu/output/ ~/server/hadoop-2.10.0/repo/hw4/

hdfs dfs -get /user/ubuntu/output/ ~/server/hadoop-2.10.0/org/


fully-distributed:
------------------


export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
PATH=$PATH:$HOME/.local/bin:$HOME/binPATH=$PATH:$JAVA_HOME/bin:PATH
export PATH
export HADOOP_HOME=/home/ubuntu/server/hadoop-2.10.0
export HADOOP_PREFIX=/home/ubuntu/server/hadoop-2.10.0
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
source ~/.bashrc

sudo nano /etc/hosts
	172.31.42.214 master
	172.31.39.227 slave1
Ctrl + S
Ctrl + D

cd ~/server/hadoop-2.10.0/etc/hadoop/

sudo nano hadoop-env.sh
	# The java implementation to use.
	export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
	export HADOOP_HOME=/home/ubuntu/server/hadoop-2.10.0

	# The jsvc implementation to use. Jsvc is required to run secure datanodes
	# that bind to privileged ports to provide authentication of data transfer
	# protocol.  Jsvc is not required if SASL is configured for authentication of
	# data transfer protocol using non-privileged ports.
	#export JSVC_HOME=${JSVC_HOME}

	export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

	# Extra Java CLASSPATH elements.  Automatically insert capacity-scheduler.
	for file in /home/ubuntu/server/hadoop-2.10.0/share/hadoop/*/*.jar

	do

		export CLASSPATH=$CLASSPATH:$file

	done

	for file in /home/ubuntu/server/hadoop-2.10.0/share/hadoop/*/lib/*.jar

	do

		export CLASSPATH=$CLASSPATH:$file

	done
Ctrl + S
Ctrl + D

sudo nano core-site.xml
	<configuration>
		<property>
			<name>fs.defaultFS</name>
			<value>hdfs://master:9000</value> 
		</property>
		<property>
			<name>hadoop.tmp.dir</name>
			<value>file:/home/ubuntu/server/hadoop-2.10.0/tmp</value> 
		</property>
		<property>
			<name>hadoop.proxyuser.hadoop.hosts</name>
			<value>*</value> 
		</property>
		<property>
			<name>hadoop.proxyuser.hadoop.groups</name>
			<value>*</value>
		</property>
	</configuration>
Ctrl + S
Ctrl + D

sudo nano hdfs-site.xml
	<configuration>
		<property>
			<name>dfs.name.dir</name>
			<value>/home/ubuntu/server/hadoop-2.10.0/dfs/name</value>
			<final>true</final>
		</property>
		<property>
			<name>dfs.data.dir</name>
			<value>/home/ubuntu/server/hadoop-2.10.0/dfs/name/data</value>
			<final>true</final>
		</property>
		<property>
			<name>dfs.replication</name>
			<value>3</value>
		</property>
	</configuration>
Ctrl + S
Ctrl + D

sudo nano mapred-site.xml
	<configuration>
		<property> 
			<name>mapred.job.tracker</name> 
			<value>master:9001</value> 
		</property>
		<property> 
			<name>mapred.framework.name</name> 
			<value>yarn</value> 
		</property>
	</configuration>
Ctrl + S
Ctrl + D

sudo nano yarn-site.xml
	<configuration>
		<property>
			<name>yarn.nodemanager.aux-services</name>
			<value>mapreduce_shuffle</value>
		</property>
		<property>
			<name>yarn.nodemanager.aux-services.shuffle.class</name>
			<value>org.apache.hadoop.mapred.ShuffleHandler</value>
		</property>
		<property>
			<name>yarn.resourcemanager.address</name>
			<value>master:8032</value>
		</property>
		<property>
			<name>yarn.resourcemanager.scheduler.address</name>
			<value>master:8030</value>
		</property>
		<property>
			<name>yarn.resourcemanager.resource-tracker.address</name>
			<value>master:8035</value>
		</property>
		<property>
			<name>yarn.resourcemanager.admin.address</name>
			<value>master:8033</value>
		</property>
		<property>
			<name>yarn.resourcemanager.webapp.address</name>
			<value>master:8088</value>
		</property>
		<property>
			<name>yarn.resourcemanager.scheduler.class</name>
			<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
		</property>
		<property>
			<name>yarn.nodemanager.vmem-check-enabled</name>
			<value>false</value>
		</property>
		<property>
			<name>yarn.resourcemanager.hostname</name>
			<value>master</value>
		</property>
	</configuration>
Ctrl + S
Ctrl + D

sudo nano slaves
	slave1
Ctrl + S
Ctrl + D

hdfs namenode -format
hadoop dfsadmin -safemode leave

start-dfs.sh
start-yarn.sh

hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/ubuntu
hdfs dfs -mkdir /user/ubuntu/input

hdfs dfs -put ~/server/hadoop-2.10.0/repo/hw4/input/Input.txt /user/ubuntu/input/

hadoop jar RelFrequency.jar /user/ubuntu/input/ /user/ubuntu/output
ubuntu@ip-172-31-42-214:~/server/hadoop-2.10.0/repo/hw4$ hdfs dfs -get /user/ubuntu/output/ ~/server/hadoop-2.10.0/repo/hw4/

hdfs dfs -get /user/ubuntu/output/ ~/server/hadoop-2.10.0/org/